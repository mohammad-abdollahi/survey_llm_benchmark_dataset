# Surveying the Benchmarking Landscape of Large Language Models in Code Intelligence

Welcome to the official repository for our survey paper:

**â€œSurveying the Benchmarking Landscape of Large Language Models in Code Intelligenceâ€**
<!-- *(January 2020 â€“ June 2025)* -->

## ğŸ“Œ Overview

With the rapid evolution of Large Language Models (LLMs) such as GPT-2, GPT-3, and their successors, there has been a transformative shift in the field of **code intelligence**, enabling significant advances in tasks like code generation, program repair, software testing, and debugging.

To ensure these models are evaluated rigorously and meaningfully, **benchmarking** plays a crucial role.

In this work, we systematically review:

* **142 research papers**
* **156 unique benchmark datasets**
* **32 different code-related tasks**

We analyze each dataset across four key dimensions:

1. **General landscape and coverage**
2. **Dataset construction and quality assurance**
3. **Evaluation protocols**
4. **Limitations and gaps**

## ğŸ” Key Findings

* **Python** is the most dominant language (used in 77% of datasets)
* **GitHub** is the primary data source (46% usage)
* Most benchmarks focus on **code generation** (86 datasets)
* Benchmark creation has notably accelerated in the past 3 years
* Gaps exist in terms of **bias**, **dataset evolution**, and **standardized evaluation**

<!-- ## ğŸ“ˆ Vision for the Future

We advocate for a new generation of benchmark datasets that are:

* More **robust** and **adaptable**
* Better aligned with **real-world coding challenges**
* Designed with **practical evaluation needs** in mind -->

## ğŸ“„ Paper Access

You can read the full survey here:
ğŸ“– \[https://hal.science/view/index/docid/5183398]

## ğŸ“š Citation

If you find this work useful in your research, please consider citing it:

```bibtex
@article{abdollahi:hal-05183398,
  TITLE = {{Surveying the Benchmarking Landscape of Large Language Models in Code Intelligence}},
  AUTHOR = {Abdollahi, Mohammad and Zhang, Ruixin and Shiri Harzevili, Nima and Shin, Jiho and Wang, Song and Hemmati, Hadi},
  URL = {https://hal.science/hal-05183398},
  NOTE = {37 pages + references},
  YEAR = {2025},
  MONTH = Jul,
  KEYWORDS = {Large language Models LLMs ; Benchmark ; Code Intelligence ; Software Engineering},
  PDF = {https://hal.science/hal-05183398v1/file/main.pdf},
  HAL_ID = {hal-05183398},
  HAL_VERSION = {v1},
}
```

<!-- ## ğŸ“ Repository Contents

* `data/` â€“ Metadata and stats from analyzed papers and benchmarks
* `figures/` â€“ Figures and charts used in the paper
* `tables/` â€“ Summary tables and benchmark listings
* `references.bib` â€“ Bibliography of the 142 surveyed papers
* `README.md` â€“ This file -->

<!-- ## ğŸ™Œ Contributions & Feedback

We welcome community feedback to improve this work.
If you find errors, have suggestions, or want to contribute extensions to the dataset or analysis, feel free to open an issue or submit a pull request. -->

<!-- ## ğŸ“§ Contact

For questions or collaboration inquiries, please contact:
**\[Your Name]**
\[Your Affiliation]
\[Your Email]

