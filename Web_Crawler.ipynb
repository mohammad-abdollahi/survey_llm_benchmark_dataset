{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83df3bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "import json\n",
    "from selenium import webdriver\n",
    "import random\n",
    "from datetime import datetime\n",
    "from urllib.parse import quote\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "def loadtxt():\n",
    "    return [line.strip() for line in open(\"query.txt\", \"r\")]\n",
    "\n",
    "def jsonWriter(data, filename):\n",
    "    with open(f'{filename}.json', 'a') as f:\n",
    "        json.dump(data, f, ensure_ascii=False)\n",
    "\n",
    "def newWriter(row, filename):\n",
    "    with open(f'{filename}.csv', 'a', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(row)\n",
    "\n",
    "def randomize_user_agent():\n",
    "    user_agents = [\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 14_3) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15',\n",
    "        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'\n",
    "    ]\n",
    "    return {'User-Agent': random.choice(user_agents)}\n",
    "\n",
    "def handle_cookie_consent():\n",
    "    try:\n",
    "        cookie_btn = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//button[contains(., 'Accept') or contains(., 'Agree') or contains(., 'OK')]\"))\n",
    "        )\n",
    "        cookie_btn.click()\n",
    "        time.sleep(2)\n",
    "    except TimeoutException:\n",
    "        pass\n",
    "\n",
    "def is_recent_publication(year_text):\n",
    "    try:\n",
    "        year = int(re.search(r'\\d{4}', year_text).group())\n",
    "        return year >= 2020\n",
    "    except (AttributeError, ValueError):\n",
    "        return False\n",
    "        \n",
    "def google_scholar_scraper(query, start=0):\n",
    "    base_url = f\"https://scholar.google.com/scholar?start={start}&q={query}&hl=en&as_sdt=0,5&as_ylo=2020\"\n",
    "    driver.get(base_url)\n",
    "    if start == 0:\n",
    "        time.sleep(20)\n",
    "    else:\n",
    "        time.sleep(random.uniform(2,4))\n",
    "    \n",
    "    try:\n",
    "        results = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.CLASS_NAME, \"gs_ri\"))\n",
    "        )\n",
    "        \n",
    "        papers = []\n",
    "        for result in results:\n",
    "            try:\n",
    "                title = result.find_element(By.TAG_NAME, 'h3').text\n",
    "                link = result.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
    "                year_text = result.find_element(By.CLASS_NAME, 'gs_a').text\n",
    "                \n",
    "                if is_recent_publication(year_text):\n",
    "                    papers.append([title, link, year_text])\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        for paper in papers:\n",
    "            newWriter(paper, 'GoogleScholarPapers')\n",
    "        \n",
    "        return len(papers)\n",
    "\n",
    "    except TimeoutException:\n",
    "        print(\"No results found or page didn't load\")\n",
    "        return 0\n",
    "\n",
    "def parse_ieee(keyword, page=0):\n",
    "    url = f\"https://ieeexplore.ieee.org/search/searchresult.jsp?queryText={keyword}&pageNumber={page}&ranges=2020_{datetime.now().year}_Year\"\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    papers = []\n",
    "    try:\n",
    "        results = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.CSS_SELECTOR, \".List-results-items\"))\n",
    "        )\n",
    "        \n",
    "        for item in results:\n",
    "            try:\n",
    "                title = item.find_element(By.CSS_SELECTOR, \"h3 a\").text\n",
    "                year_text = item.find_element(By.XPATH, \".//*[contains(text(), 'Year:')]\").text\n",
    "                \n",
    "                if is_recent_publication(year_text):\n",
    "                    papers.append([title, f\"IEEE {year_text}\"])\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        for paper in papers:\n",
    "            newWriter(paper, 'IEEEPapers')\n",
    "        \n",
    "        return len(papers)\n",
    "    \n",
    "    except TimeoutException:\n",
    "        print(\"No IEEE results found\")\n",
    "        return 0\n",
    "\n",
    "def parse_science_direct(keyword, offset=0):\n",
    "    \"\"\"Scrapes ScienceDirect search results for paper titles and links.\"\"\"\n",
    "    \n",
    "    url = f'https://www.sciencedirect.com/search?date=2020-2025&tak={keyword}&offset={offset}'\n",
    "    \n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    handle_cookie_consent()\n",
    "\n",
    "    papers = []\n",
    "    \n",
    "    try:\n",
    "        # Wait until search results load\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.CLASS_NAME, \"result-item-container\"))\n",
    "        )\n",
    "        \n",
    "        results = driver.find_elements(By.CLASS_NAME, \"result-item-container\")\n",
    "\n",
    "        for item in results:\n",
    "            try:\n",
    "                title_element = item.find_element(By.TAG_NAME, 'h2').find_element(By.TAG_NAME, 'a')\n",
    "                title = title_element.text\n",
    "                link = title_element.get_attribute('href')\n",
    "                papers.append([title, link])\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping item due to error: {e}\")\n",
    "                continue\n",
    "        \n",
    "        for paper in papers:\n",
    "            newWriter(paper, 'ScienceDirectPapers')\n",
    "        \n",
    "        return len(papers)\n",
    "    \n",
    "    except TimeoutException:\n",
    "        print(\"No ScienceDirect results found\")\n",
    "        return 0\n",
    "\n",
    "def parse_acm(query, page):\n",
    "    #url = f\"https://dl.acm.org/action/doSearch?field1=TitleAbstract&text1={query}&AfterYear=2020&startPage={page-1}&pageSize=50\"\n",
    "    #url = f\"https://dl.acm.org/action/doSearch?fillQuickSearch=false&target=advanced&expand=dl&AfterYear=2020&AllField={query}&pageSize=20&startPage={page-1}\"\n",
    "    url = f'https://dl.acm.org/action/doSearch?fillQuickSearch=false&target=advanced&expand=dl&field1=Title&text1={query}&field2=Abstract&text2={query}&AfterYear=2020&pageSize=20&startPage={page-1}'\n",
    "    #'https://dl.acm.org/action/doSearch?fillQuickSearch=false&target=advanced&expand=dl&AfterYear=2020&AllField={query}&pageSize=20&startPage={page-1}'\n",
    "    #https://dl.acm.org/action/doSearch?fillQuickSearch=false&target=advanced&expand=dl&AfterYear=2020&AllField=+AND+AllField%3A%28benchmark+dataset+code+generation+llm%29&pageSize=20&startPage=1\n",
    "    response = requests.get(url, headers=randomize_user_agent())\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    papers = []\n",
    "    for item in soup.select('.issue-item__title'):\n",
    "        try:\n",
    "            title = item.text.strip()\n",
    "            link = \"https://dl.acm.org\" + item.find('a')['href']\n",
    "            year_text = item.find_next(class_='bookPubDate').text\n",
    "            \n",
    "            if is_recent_publication(year_text):\n",
    "                papers.append([title, link, year_text])\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    for paper in papers:\n",
    "        newWriter(paper, 'ACMPapers')\n",
    "    \n",
    "    print(f'ACM: {len(papers)}')\n",
    "    return len(papers)\n",
    "\n",
    "def main():\n",
    "    queries = [\n",
    "        'Benchmark+Dataset+Code+Generation+LLM',\n",
    "        'Benchmark+Dataset+Code+Summarization+LLM',\n",
    "        'Benchmark+Dataset+Code+Test+Cases+Generation+LLM',\n",
    "        'Benchmark+Dataset+Patch+Generation+LLM',\n",
    "        'Benchmark+Dataset+Code+Optimization+LLM',\n",
    "        'Benchmark+Dataset+Code+Translation+LLM',\n",
    "        'Benchmark+Dataset+Program+Repair+LLM',\n",
    "        'Benchmark+Dataset+Requirement+Generation+LLM',\n",
    "        'Benchmark+Dataset+Software+Development+LLM',\n",
    "        'Benchmark+Dataset+Software+Engineering+LLM',\n",
    "        'Benchmark+Dataset+Code+Review+LLM',\n",
    "        'Benchmark+Dataset+Code+Generation+Large+Language+Model',\n",
    "        'Benchmark+Dataset+Code+Summarization+Large+Language+Model',\n",
    "        'Benchmark+Dataset+Code+Test+Cases+Generation+Large+Language+Model',\n",
    "        'Benchmark+Dataset+Patch+Generation+Large+Language+Model',\n",
    "        'Benchmark+Dataset+Code+Optimization+Large+Language+Model',\n",
    "        'Benchmark+Dataset+Code+Translation+Large+Language+Model',\n",
    "        'Benchmark+Dataset+Program+Repair+Large+Language+Model',\n",
    "        'Benchmark+Dataset+Requirement+Generation+Large+Language+Model',\n",
    "        'Benchmark+Dataset+Software+Development+Large+Language+Model',\n",
    "        'Benchmark+Dataset+Software+Engineering+Large+Language+Model',\n",
    "        'Benchmark+Dataset+Code+Review+Large+Language+Model'\n",
    "        ]\n",
    "    \n",
    "    for query in queries:\n",
    "        print(f\"Processing: {query}\")\n",
    "        \n",
    "        # Google Scholar\n",
    "        count_total = 0\n",
    "        for page in range(0, 20):  # First 5 pages\n",
    "            count = google_scholar_scraper(query, start=page*10)\n",
    "            count_total+=count\n",
    "            if count == 0: \n",
    "                print(count_total)\n",
    "                print(f'number of papers for google scholar: {count_total}')\n",
    "                break\n",
    "            if page == 19:\n",
    "                print(f'number of papers for google scholar: {count_total}')\n",
    "        \n",
    "        # IEEE\n",
    "        #count_total = 0\n",
    "        #for page in range(0, 20):  # First 5 pages\n",
    "            #count = parse_ieee(query, page)\n",
    "            #count_total+=count\n",
    "            #if count == 0: \n",
    "                #print(f'number of papers for IEEE: {count_total}')\n",
    "                #break\n",
    "\n",
    "        # ScienceDirect\n",
    "        #count_total = 0\n",
    "        #for offset in range(0, 250, 25):  # First 5 pages\n",
    "            #count = parse_science_direct(query, offset)\n",
    "            #count_total+=count\n",
    "            #if count == 0:  \n",
    "                #print(f'number of papers for science direct: {count_total}') \n",
    "                #break\n",
    "        \n",
    "        # ACM\n",
    "        count_total = 0\n",
    "        for page in range(1, 6):  # First 5 pages\n",
    "            count = parse_acm(query, page)\n",
    "            count_total+=count\n",
    "            if count == 0: \n",
    "                print(count_total)\n",
    "                print(f'number of papers for google scholar: {count_total}')\n",
    "                break\n",
    "            if page == 11:\n",
    "                print(f'number of papers for google scholar: {count_total}')\n",
    "    \n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a4cb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "import json\n",
    "from selenium import webdriver\n",
    "import random\n",
    "from datetime import datetime\n",
    "from urllib.parse import quote\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "def loadtxt():\n",
    "    return [line.strip() for line in open(\"query.txt\", \"r\")]\n",
    "\n",
    "def jsonWriter(data, filename):\n",
    "    with open(f'{filename}.json', 'a') as f:\n",
    "        json.dump(data, f, ensure_ascii=False)\n",
    "\n",
    "def newWriter(row, filename):\n",
    "    with open(f'{filename}.csv', 'a', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(row)\n",
    "\n",
    "def randomize_user_agent():\n",
    "    user_agents = [\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 14_3) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15',\n",
    "        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'\n",
    "    ]\n",
    "    return {'User-Agent': random.choice(user_agents)}\n",
    "\n",
    "def handle_cookie_consent():\n",
    "    try:\n",
    "        cookie_btn = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//button[contains(., 'Accept') or contains(., 'Agree') or contains(., 'OK')]\"))\n",
    "        )\n",
    "        cookie_btn.click()\n",
    "        time.sleep(2)\n",
    "    except TimeoutException:\n",
    "        pass\n",
    "\n",
    "def is_recent_publication(year_text):\n",
    "    try:\n",
    "        year = int(re.search(r'\\d{4}', year_text).group())\n",
    "        return year >= 2020\n",
    "    except (AttributeError, ValueError):\n",
    "        return False\n",
    "        \n",
    "def google_scholar_scraper(query, start=0):\n",
    "    base_url = f\"https://scholar.google.com/scholar?start={start}&q={query}&hl=en&as_sdt=0,5&as_ylo=2020\"\n",
    "    driver.get(base_url)\n",
    "    if start == 0:\n",
    "        time.sleep(30)\n",
    "    else:\n",
    "        time.sleep(random.uniform(2,4))\n",
    "    \n",
    "    try:\n",
    "        results = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.CLASS_NAME, \"gs_ri\"))\n",
    "        )\n",
    "        \n",
    "        papers = []\n",
    "        for result in results:\n",
    "            try:\n",
    "                title = result.find_element(By.TAG_NAME, 'h3').text\n",
    "                link = result.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
    "                year_text = result.find_element(By.CLASS_NAME, 'gs_a').text\n",
    "                \n",
    "                if is_recent_publication(year_text):\n",
    "                    papers.append([title, link, year_text])\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        for paper in papers:\n",
    "            newWriter(paper, 'GoogleScholarPapers')\n",
    "        \n",
    "        return len(papers)\n",
    "\n",
    "    except TimeoutException:\n",
    "        print(\"No results found or page didn't load\")\n",
    "        return 0\n",
    "\n",
    "def parse_ieee(keyword, page=0):\n",
    "    url = f\"https://ieeexplore.ieee.org/search/searchresult.jsp?queryText={keyword}&pageNumber={page}&ranges=2020_{datetime.now().year}_Year\"\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    papers = []\n",
    "    try:\n",
    "        results = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.CSS_SELECTOR, \".List-results-items\"))\n",
    "        )\n",
    "        \n",
    "        for item in results:\n",
    "            try:\n",
    "                title = item.find_element(By.CSS_SELECTOR, \"h3 a\").text\n",
    "                year_text = item.find_element(By.XPATH, \".//*[contains(text(), 'Year:')]\").text\n",
    "                \n",
    "                if is_recent_publication(year_text):\n",
    "                    papers.append([title, f\"IEEE {year_text}\"])\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        for paper in papers:\n",
    "            newWriter(paper, 'IEEEPapers')\n",
    "        \n",
    "        return len(papers)\n",
    "    \n",
    "    except TimeoutException:\n",
    "        print(\"No IEEE results found\")\n",
    "        return 0\n",
    "\n",
    "def parse_science_direct(keyword, offset=0):\n",
    "    \"\"\"Scrapes ScienceDirect search results for paper titles and links.\"\"\"\n",
    "    \n",
    "    url = f'https://www.sciencedirect.com/search?date=2020-2025&tak={keyword}&offset={offset}'\n",
    "    \n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    handle_cookie_consent()\n",
    "\n",
    "    papers = []\n",
    "    \n",
    "    try:\n",
    "        # Wait until search results load\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.CLASS_NAME, \"result-item-container\"))\n",
    "        )\n",
    "        \n",
    "        results = driver.find_elements(By.CLASS_NAME, \"result-item-container\")\n",
    "\n",
    "        for item in results:\n",
    "            try:\n",
    "                title_element = item.find_element(By.TAG_NAME, 'h2').find_element(By.TAG_NAME, 'a')\n",
    "                title = title_element.text\n",
    "                link = title_element.get_attribute('href')\n",
    "                papers.append([title, link])\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping item due to error: {e}\")\n",
    "                continue\n",
    "        \n",
    "        for paper in papers:\n",
    "            newWriter(paper, 'ScienceDirectPapers')\n",
    "        \n",
    "        return len(papers)\n",
    "    \n",
    "    except TimeoutException:\n",
    "        print(\"No ScienceDirect results found\")\n",
    "        return 0\n",
    "\n",
    "def parse_acm(query, page):\n",
    "    #url = f\"https://dl.acm.org/action/doSearch?field1=TitleAbstract&text1={query}&AfterYear=2020&startPage={page-1}&pageSize=50\"\n",
    "    url = f\"https://dl.acm.org/action/doSearch?fillQuickSearch=false&target=advanced&expand=dl&AfterYear=2020&ABSTRACT={query}&pageSize=20&startPage={page-1}\"\n",
    "    #url = f\"https://dl.acm.org/action/doSearch?field1=TitleAbstract&text1={query}&AfterYear=2020&startPage={page-1}&pageSize=20\"#f'https://dl.acm.org/action/doSearch?fillQuickSearch=false&target=advanced&expand=dl&field1=Title&text1={query}&field2=Abstract&text2={query}&AfterYear=2020&pageSize=20&startPage={page-1}'\n",
    "    #'https://dl.acm.org/action/doSearch?fillQuickSearch=false&target=advanced&expand=dl&AfterYear=2020&AllField={query}&pageSize=20&startPage={page-1}'\n",
    "    #https://dl.acm.org/action/doSearch?fillQuickSearch=false&target=advanced&expand=dl&AfterYear=2020&AllField=+AND+AllField%3A%28benchmark+dataset+code+generation+llm%29&pageSize=20&startPage=1\n",
    "    response = requests.get(url, headers=randomize_user_agent())\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    papers = []\n",
    "    for item in soup.select('.issue-item__title'):\n",
    "        try:\n",
    "            title = item.text.strip()\n",
    "            link = \"https://dl.acm.org\" + item.find('a')['href']\n",
    "            year_text = item.find_next(class_='bookPubDate').text\n",
    "            \n",
    "            if is_recent_publication(year_text):\n",
    "                papers.append([title, link, year_text])\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    for paper in papers:\n",
    "        newWriter(paper, 'ACMPapers')\n",
    "    \n",
    "    print(f'ACM: {len(papers)}')\n",
    "    return len(papers)\n",
    "\n",
    "def main():\n",
    "    queries = [\n",
    "        'Benchmark+Dataset+Code+Generation+LLM',\n",
    "        'Benchmark+Dataset+Code+Summarization+LLM',\n",
    "        'Benchmark+Dataset+Code+Test+Cases+Generation+LLM',\n",
    "        'Benchmark+Dataset+Patch+Generation+LLM',\n",
    "        'Benchmark+Dataset+Code+Optimization+LLM',\n",
    "        'Benchmark+Dataset+Code+Translation+LLM',\n",
    "        'Benchmark+Dataset+Program+Repair+LLM',\n",
    "        'Benchmark+Dataset+Requirement+Generation+LLM',\n",
    "        'Benchmark+Dataset+Software+Development+LLM',\n",
    "        'Benchmark+Dataset+Software+Engineering+LLM',\n",
    "        'Benchmark+Dataset+Code+Review+LLM',\n",
    "        'Benchmark+Dataset+Code+Generation+Large+Language+Model',\n",
    "        'Benchmark+Dataset+Code+Summarization+Large+Language+Model',\n",
    "        'Benchmark+Dataset+Code+Test+Cases+Generation+Large+Language+Model',\n",
    "        'Benchmark+Dataset+Patch+Generation+Large+Language+Model',\n",
    "        'Benchmark+Dataset+Code+Optimization+Large+Language+Model',\n",
    "        'Benchmark+Dataset+Code+Translation+Large+Language+Model',\n",
    "        'Benchmark+Dataset+Program+Repair+Large+Language+Model',\n",
    "        'Benchmark+Dataset+Requirement+Generation+Large+Language+Model',\n",
    "        'Benchmark+Dataset+Software+Development+Large+Language+Model',\n",
    "        'Benchmark+Dataset+Software+Engineering+Large+Language+Model',\n",
    "        'Benchmark+Dataset+Code+Review+Large+Language+Model'\n",
    "        ]\n",
    "    \n",
    "    for query in queries:\n",
    "        print(f\"Processing: {query}\")\n",
    "        \n",
    "        # Google Scholar\n",
    "        count_total = 0\n",
    "        for page in range(0, 6):  # First 5 pages\n",
    "            count = google_scholar_scraper(query, start=page*10)\n",
    "            count_total+=count\n",
    "            if count == 0: \n",
    "                print(count_total)\n",
    "                print(f'number of papers for google scholar: {count_total}')\n",
    "                break\n",
    "            if page == 5:\n",
    "                print(f'number of papers for google scholar: {count_total}')\n",
    "        \n",
    "        # IEEE\n",
    "        count_total = 0\n",
    "        for page in range(0, 20):  # First 5 pages\n",
    "            count = parse_ieee(query, page)\n",
    "            count_total+=count\n",
    "            if count == 0: \n",
    "                print(f'number of papers for IEEE: {count_total}')\n",
    "                break\n",
    "\n",
    "        # ScienceDirect\n",
    "        count_total = 0\n",
    "        for offset in range(0, 250, 25):  # First 5 pages\n",
    "            count = parse_science_direct(query, offset)\n",
    "            count_total+=count\n",
    "            if count == 0:  \n",
    "                print(f'number of papers for science direct: {count_total}') \n",
    "                break\n",
    "        \n",
    "        # ACM\n",
    "        count_total = 0\n",
    "        for page in range(1, 6):  # First 5 pages\n",
    "            count = parse_acm(query, page)\n",
    "            count_total+=count\n",
    "            if count == 0: \n",
    "                print(count_total)\n",
    "                print(f'number of papers for google scholar: {count_total}')\n",
    "                break\n",
    "            if page == 5:\n",
    "                print(f'number of papers for google scholar: {count_total}')\n",
    "    \n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f1fbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "import json\n",
    "from selenium import webdriver\n",
    "import random\n",
    "from datetime import datetime\n",
    "from urllib.parse import quote\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "def loadtxt():\n",
    "    return [line.strip() for line in open(\"query.txt\", \"r\")]\n",
    "\n",
    "def jsonWriter(data, filename):\n",
    "    with open(f'{filename}.json', 'a') as f:\n",
    "        json.dump(data, f, ensure_ascii=False)\n",
    "\n",
    "def newWriter(row, filename):\n",
    "    with open(f'{filename}.csv', 'a', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(row)\n",
    "\n",
    "def randomize_user_agent():\n",
    "    user_agents = [\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 14_3) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15',\n",
    "        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'\n",
    "    ]\n",
    "    return {'User-Agent': random.choice(user_agents)}\n",
    "\n",
    "def handle_cookie_consent():\n",
    "    try:\n",
    "        cookie_btn = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//button[contains(., 'Accept') or contains(., 'Agree') or contains(., 'OK')]\"))\n",
    "        )\n",
    "        cookie_btn.click()\n",
    "        time.sleep(2)\n",
    "    except TimeoutException:\n",
    "        pass\n",
    "\n",
    "def is_recent_publication(year_text):\n",
    "    try:\n",
    "        year = int(re.search(r'\\d{4}', year_text).group())\n",
    "        return year >= 2020\n",
    "    except (AttributeError, ValueError):\n",
    "        return False\n",
    "        \n",
    "def google_scholar_scraper(query, start=0):\n",
    "    base_url = f\"https://scholar.google.com/scholar?start={start}&q={query}&hl=en&as_sdt=0,5&as_ylo=2020\"\n",
    "    driver.get(base_url)\n",
    "    if start == 0:\n",
    "        time.sleep(30)\n",
    "    else:\n",
    "        time.sleep(random.uniform(2,4))\n",
    "    \n",
    "    try:\n",
    "        results = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.CLASS_NAME, \"gs_ri\"))\n",
    "        )\n",
    "        \n",
    "        papers = []\n",
    "        for result in results:\n",
    "            try:\n",
    "                title = result.find_element(By.TAG_NAME, 'h3').text\n",
    "                link = result.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
    "                year_text = result.find_element(By.CLASS_NAME, 'gs_a').text\n",
    "                \n",
    "                if is_recent_publication(year_text):\n",
    "                    papers.append([title, link, year_text])\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        for paper in papers:\n",
    "            newWriter(paper, 'GoogleScholarPapers')\n",
    "        \n",
    "        return len(papers)\n",
    "\n",
    "    except TimeoutException:\n",
    "        print(\"No results found or page didn't load\")\n",
    "        return 0\n",
    "\n",
    "def parse_ieee(keyword, page=0):\n",
    "    url = f\"https://ieeexplore.ieee.org/search/searchresult.jsp?queryText={keyword}&pageNumber={page}&ranges=2020_{datetime.now().year}_Year\"\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    papers = []\n",
    "    try:\n",
    "        results = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.CSS_SELECTOR, \".List-results-items\"))\n",
    "        )\n",
    "        \n",
    "        for item in results:\n",
    "            try:\n",
    "                title = item.find_element(By.CSS_SELECTOR, \"h3 a\").text\n",
    "                year_text = item.find_element(By.XPATH, \".//*[contains(text(), 'Year:')]\").text\n",
    "                \n",
    "                if is_recent_publication(year_text):\n",
    "                    papers.append([title, f\"IEEE {year_text}\"])\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        for paper in papers:\n",
    "            newWriter(paper, 'IEEEPapers')\n",
    "        \n",
    "        return len(papers)\n",
    "    \n",
    "    except TimeoutException:\n",
    "        print(\"No IEEE results found\")\n",
    "        return 0\n",
    "\n",
    "def parse_science_direct(keyword, offset=0):\n",
    "    \"\"\"Scrapes ScienceDirect search results for paper titles and links.\"\"\"\n",
    "    \n",
    "    url = f'https://www.sciencedirect.com/search?date=2020-2025&tak={keyword}&offset={offset}'\n",
    "    \n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    handle_cookie_consent()\n",
    "\n",
    "    papers = []\n",
    "    \n",
    "    try:\n",
    "        # Wait until search results load\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.CLASS_NAME, \"result-item-container\"))\n",
    "        )\n",
    "        \n",
    "        results = driver.find_elements(By.CLASS_NAME, \"result-item-container\")\n",
    "\n",
    "        for item in results:\n",
    "            try:\n",
    "                title_element = item.find_element(By.TAG_NAME, 'h2').find_element(By.TAG_NAME, 'a')\n",
    "                title = title_element.text\n",
    "                link = title_element.get_attribute('href')\n",
    "                papers.append([title, link])\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping item due to error: {e}\")\n",
    "                continue\n",
    "        \n",
    "        for paper in papers:\n",
    "            newWriter(paper, 'ScienceDirectPapers')\n",
    "        \n",
    "        return len(papers)\n",
    "    \n",
    "    except TimeoutException:\n",
    "        print(\"No ScienceDirect results found\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "def parse_acm(query, page):\n",
    "    #url = f\"https://dl.acm.org/action/doSearch?field1=TitleAbstract&text1={query}&AfterYear=2020&startPage={page-1}&pageSize=50\"\n",
    "    url = f\"https://dl.acm.org/action/doSearch?fillQuickSearch=false&target=advanced&expand=dl&AfterYear=2020&TITLE={query}&pageSize=20&startPage={page-1}\"\n",
    "    #url = f\"https://dl.acm.org/action/doSearch?field1=TitleAbstract&text1={query}&AfterYear=2020&startPage={page-1}&pageSize=20\"#f'https://dl.acm.org/action/doSearch?fillQuickSearch=false&target=advanced&expand=dl&field1=Title&text1={query}&field2=Abstract&text2={query}&AfterYear=2020&pageSize=20&startPage={page-1}'\n",
    "    #'https://dl.acm.org/action/doSearch?fillQuickSearch=false&target=advanced&expand=dl&AfterYear=2020&AllField={query}&pageSize=20&startPage={page-1}'\n",
    "    #https://dl.acm.org/action/doSearch?fillQuickSearch=false&target=advanced&expand=dl&AfterYear=2020&AllField=+AND+AllField%3A%28benchmark+dataset+code+generation+llm%29&pageSize=20&startPage=1\n",
    "    response = requests.get(url, headers=randomize_user_agent())\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    papers = []\n",
    "    for item in soup.select('.issue-item__title'):\n",
    "        try:\n",
    "            title = item.text.strip()\n",
    "            link = \"https://dl.acm.org\" + item.find('a')['href']\n",
    "            year_text = item.find_next(class_='bookPubDate').text\n",
    "            \n",
    "            if is_recent_publication(year_text):\n",
    "                papers.append([title, link, year_text])\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    for paper in papers:\n",
    "        newWriter(paper, 'ACMPapers')\n",
    "    \n",
    "    print(f'ACM: {len(papers)}')\n",
    "    return len(papers)\n",
    "\n",
    "def main():\n",
    "    queries = [\n",
    "        'Benchmark+Dataset+Code+Generation+LLM',\n",
    "        'Benchmark+Dataset+Code+Summarization+LLM',\n",
    "        'Benchmark+Dataset+Code+Test+Cases+Generation+LLM',\n",
    "        'Benchmark+Dataset+Patch+Generation+LLM',\n",
    "        'Benchmark+Dataset+Code+Optimization+LLM',\n",
    "        'Benchmark+Dataset+Code+Translation+LLM',\n",
    "        'Benchmark+Dataset+Program+Repair+LLM',\n",
    "        'Benchmark+Dataset+Requirement+Generation+LLM',\n",
    "        'Benchmark+Dataset+Software+Development+LLM',\n",
    "        'Benchmark+Dataset+Software+Engineering+LLM',\n",
    "        'Benchmark+Dataset+Code+Review+LLM',\n",
    "        'Benchmark+Dataset+Code+Generation+Large+Language+Model',\n",
    "        'Benchmark+Dataset+Code+Summarization+Large+Language+Model',\n",
    "        'Benchmark+Dataset+Code+Test+Cases+Generation+Large+Language+Model',\n",
    "        'Benchmark+Dataset+Patch+Generation+Large+Language+Model',\n",
    "        'Benchmark+Dataset+Code+Optimization+Large+Language+Model',\n",
    "        'Benchmark+Dataset+Code+Translation+Large+Language+Model',\n",
    "        'Benchmark+Dataset+Program+Repair+Large+Language+Model',\n",
    "        'Benchmark+Dataset+Requirement+Generation+Large+Language+Model',\n",
    "        'Benchmark+Dataset+Software+Development+Large+Language+Model',\n",
    "        'Benchmark+Dataset+Software+Engineering+Large+Language+Model',\n",
    "        'Benchmark+Dataset+Code+Review+Large+Language+Model'\n",
    "        ]\n",
    "    \n",
    "    for query in queries:\n",
    "        print(f\"Processing: {query}\")\n",
    "        \n",
    "        # Google Scholar\n",
    "        count_total = 0\n",
    "        for page in range(0, 6):  # First 5 pages\n",
    "            count = google_scholar_scraper(query, start=page*10)\n",
    "            count_total+=count\n",
    "            if count == 0: \n",
    "                print(count_total)\n",
    "                print(f'number of papers for google scholar: {count_total}')\n",
    "                break\n",
    "            if page == 5:\n",
    "                print(f'number of papers for google scholar: {count_total}')\n",
    "        \n",
    "        # IEEE\n",
    "        count_total = 0\n",
    "        for page in range(0, 20):  # First 5 pages\n",
    "            count = parse_ieee(query, page)\n",
    "            count_total+=count\n",
    "            if count == 0: \n",
    "                print(f'number of papers for IEEE: {count_total}')\n",
    "                break\n",
    "\n",
    "        # ScienceDirect\n",
    "        count_total = 0\n",
    "        for offset in range(0, 250, 25):  # First 5 pages\n",
    "            count = parse_science_direct(query, offset)\n",
    "            count_total+=count\n",
    "            if count == 0:  \n",
    "                print(f'number of papers for science direct: {count_total}') \n",
    "                break\n",
    "        \n",
    "        # ACM\n",
    "        count_total = 0\n",
    "        for page in range(1, 6):  # First 5 pages\n",
    "            count = parse_acm(query, page)\n",
    "            count_total+=count\n",
    "            if count == 0: \n",
    "                print(count_total)\n",
    "                print(f'number of papers for google scholar: {count_total}')\n",
    "                break\n",
    "            if page == 5:\n",
    "                print(f'number of papers for google scholar: {count_total}')\n",
    "    \n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
